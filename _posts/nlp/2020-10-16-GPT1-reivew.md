---
title:  "GPT1 paper review (작성 중)"
excerpt: "GPT1 리뷰/공부/정리"
toc: true
toc_sticky: true
comments: true
permalink: /project/nlp/gpt1/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling
  - Need to Complete

use_math: true
last_modified_at: 2020-10-16
---

이번 스터디 순서는 GPT-1이다. 원문은 [이곳](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)을 참고하자.

Keypoint:
- *generative pre-training*
- *discriminative fine-tuning*
- *task-aware input transformation*

# 1. Introduction

- 단어 단위 이상의 정보를 unlabeled text에 얻는 것은 다음과 같은 두 가지 이유 때문에 매우 어려움
    - text representation에서 transfer learning을 위해 가장 효과적인 optimization objective가 무엇인지 불분명하다.
        - ELMo같은 LM부터, NMT, discourse coherence와 같은 다양한 task를 통해 성공적인 representation을 얻어냈다.
    - 이렇게 얻은 표현에 대해 target task로 transfer learning을 진행할 때, 가장 효과적인 방법에 대한 합의(consensus)가 없다.
        - 이에는 (1) 모델의 아키텍쳐를 변화시키거나, 트랜스퍼 러닝을 할 수 있는 보조격의 목적함수를 추가하는 방법이 있다.
- GPT-1은 language understanding task를 위한 semi-supervised approach를 탐구한다. 
    - 이를 위해 unsupervised pre-training과 supervised fine-tuning을 활용
    - 본 논문은 unlabeled large corpus와 직접 annotate한 데이터 셋 (target tasks)를 활용하는 상황을 가정
    - target tasks는 unlabeled corpus와 같은 domain에 있지 않아도 됨 
- 본 작업은 two-stage training procedure를 갖는데, 
    - 먼저 unlabeled data에 대해 language modeling objective를 사용하여 initial parameter를 얻고,
    - 이후 이를 supervised objective에 transfer한다.
- GPT-1은 transformer를 사용
- 또한 transfer learning을 위해 ***traversal-style approaches***의 task-specific input adaptation을 활용
    - 이는 구조화된 text input을 하나의 contiguous sequence of tokens로 만드는 것
    - 앞서 언급했듯 fine-tune을 효과적이고, 모델에서 최소한의 변경을 가능하게 함

***Traversal-style Approaches:*** [원문](https://arxiv.org/pdf/1509.06664.pdf)과 같이 entailment과 같은 작업에서 premise와 hypothesis를 묶어 놓는 것을 의미
{: .notice--info}

# 2. Related Work

## Semi-supervised learning for NLP

- GPT-1이 속하는 분야로, word-level/phrase-level까지 다양하게 이용되어옴
    - e.g. word2vec, GloVe, Doc2vecv, etc
- 최근들어 unlabeled corpora을 이용해서 학습하는 방법은 최근 들어 유행하기 시작함
- 그러나 이러한 접근법들은 GPT-1이 높은 수준의 의미를 포착하려는 것과는 다르게 주로 word-level에서 transfer가 일어남

## Unsupervised pre-training

- Unsupervised pre-training는 supurvised learning objective를 수정하는 대신 좋은 시작점을 찾는 것을 목표로 하는 곳에서 semi-supervised learning의 특수한 경우
- 일종의 regularization 역할로, 일반화에 도움을 주었지만, 지금은 neural net 학습 과정에 도움되는 방법론이 개발
- GPT-1은 natural language inference, paraphrase dectection, story completion과 같은 다양한 범위에서의 효과가 있음
- ELMo나 다른 연구들은 pre-trained LM이나 machine translation model로 부터 hidden representation을 얻고, 이를 보조적인 feature(auxiliary feature)로 사용함
    - 이는 추가적인 parameter를 학습해야 함
    - 그러나 GPT-1은 모델의 최소한의 변경만을 요구

**Semi-supervised/Unsupervised pre-training의 차이:** 이 부분에서는 semi-supervised learning for NLP, Unsupervised pre-training에 대한 명확한 차이를 잘 모르겠다. 스터디를 함께 진행하시는 분이 [정리한 글](https://github.com/abooundev/nlp_paper/issues/1)이 있기는 한데, 보더라도 잘 이해가 되진 않는다. 약간 NLP에서 transfer learning이 막 시도되는 시기라서 terminology에 대한 정리가 안된 것 같기도 하다.
{: .notice--danger}

## Auxiliary training objectives

- ***Auxiliary training objectives***는 보조적인 unsupurvised training objective를 추가하는 것은 semi-supervised learning의 일종으로 보면 된다
    - 즉, 원래 학습 목적과는 별개로 목적함수에 추가하는 것을 의미
- 실험에서도 사용하긴 했지만, 이와 별개로 unsupervised pre-training이 target task와 연관있는 언어적 요소(linguistic aspect)를 배우는 것을 확인

***Auxiliary training objectives:*** BERT를 아직 배우진 않았지만 BERT의 경우에는 next sentence를 예측하거나 mask 씌운 단어를 예측한다. 이 경우 이 둘의 loss를 합치게 되는데, GPT-1에서 말하는 auxilirary는 실제 목적과는 무관하지만, loss에 추가하여 이득 보는 것을 의미한다.
{: .notice--info}


# 3. Framework

- 앞서 말했듯 LM을 배우고, fine-tune하는 형태로 진행

## 3.1 Unsupervised pre-training

- Unsupervised corpus tokens $\mathcal U = {u _1, ..., u _n}$에 대해, 일반적인 LM ojbective를 사용하여 다음의 likelihood를 최대화

$$
\begin{align}
L_1(\mathcal U) = \sum _i log P (u _i \lvert u _{i-k}, ..., u _{i-1}; \Theta)
\end{align}
$$

- **k는 context window**, conditional probability $P$는 parameter $\Theta$를 이용한 neural net임

***context window:*** word2vec같은 경우 context vector를 설정하고, center word 전후로 window size를 condition으로 주게된다. 지금은 context window가 이전 시점 밖에 없으므로 Auto-regressive하게 도는 것으로 보이는데, 이 경우 context window가 dynamic하게 도는지 확인할 필요가 있어보인다.
{: .notice--danger}

- 실험에서는 ***multi-layer transformer decoder***를 사용
- input context token - multi-layer transformer decoder - position-wise feedforward 순서로 진행

$$
\begin{align}
& h _0 = UW _e + W _p \\
& h _l = \textrm{transformer_block} (h _{l-1}) \forall \in [1, n] \\
& p(U) = \textrm{softmax}(h _n W _e ^T) \\
\end{align}
$$

- $ U = (u _{-k}, ..., u {-1}$이고, $n$은 layer의 수, $W _e$는 token embedding matrix, $W _p$는 position embedding matrix

***multi-layer transformer decoder:*** Citation이 걸린 [논문](https://arxiv.org/pdf/1509.06664.pdf)을 확인해보았는데, *local attention*과 *memory-compressed attention*이라는 개념을 소개하고 있다. GPT-1에서도 이러한 개념을 사용하는지는 의문.
{: .notice--danger}

## 3.2 Supervised fine-tuning

- Eq. 1의 objective를 따라 training한 후 supervised target task에 transfer하면 됨
- 여기선 labeled dataset $\mathcal C$를 가정
    - $x^1, ..., x^m$과 같은 sequence of input token과 label $y$로 이루어짐
- input은 pre-trained model을 지나 마지막 transformer block에서 $h^m _l$을 얻고, linear output layer $W _y$를 통해 $y$를 예측

$$
P(y \rvert x^1, ..., x^m) = \textrm{softmax}(h^m _l W_y)
$$

- 이는 다음 objective를 maximize하게 한다

$$
L _2 (\mathcal C) = \sum _{(x, y)} log P(y \rvert x^1, ..., x^m)
$$

- 추가적으로, LM을 auxiliary objective를 fine-tuning에 포함하는 것이 도움이 되는 것을 발견
    - supervised model의 generalization을 향상
    - convergence를 빠르게 도와줌
- 구체적으로는 다음과 같은 objective를 optimize (with weight $\lambda$)

$$
L_3(\mathcal C) = L _2(\mathcal C) + \lambda * L _1(\mathcal C)
$$

-fine-tuning단계에서 추가적으로 학습할 parameter는 $W_y$와 delimiter token을 위한 embedding밖에 없음

![figure 1](https://user-images.githubusercontent.com/47516855/96349181-c21bf000-10e8-11eb-9dd1-2bfa44badfc0.png){: width="800"}{: .align-center}

## 3.3 Task-specific input transformations

- 어떤 tasks에선 위에서 언급한 바와 같이 직접적으로 fine-tune할 수 있음
- 그러나 QA/textual entailment같은 structured input은 delimiter token을 통해 sequence를 합침 (i.e. traversal-style approach)

# 4. Experiments

## 4.1 Setup

- 12개의 transformer layers
- 12개의 head (하나 당 64dim)
- position-wise FFN의 size는 3072
- Adam optimization
    - 0부터 2000까지 max lr: 2.5e-4
    - 그 후 ***cosine schedule***을 이용하여 0까지 annealing
- 100 epochs, 64 mini-batch
- 512개의 contiguous tokens
- weight init: $N(0, 0.02)$
    - layer norm이 많이 사용되었기 때문에 이거로도 충분함
- BPE: 40000 tokens
- dropout(p=0.1)
- ***L2 regularization ($w=0.01$)***
- GELU activation

***cosine schedule***
{: .notice--danger}

***L2 regularization ($w=0.01$):*** [원문](https://openreview.net/forum?id=rk6qdGgCZ)/[Revision version으로 보이는 논문](https://arxiv.org/abs/1711.05101)을 보면, SGD와는 달리 Adam에서 L2 regularization과 weight decay는 서로 다른 것이라고 한다. 다음을 참고해보자. [AdamW에 대해 알아보자! Decoupled weight decay regularization 논문 리뷰(1)](https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html). [다음](https://gist.github.com/sksq96/eb0174cd25b74b48c7e23f647a6dab66)은 pytorch 구현이다.
{: .notice--danger}

