---
title:  "GPT1 paper review (작성 중)"
excerpt: "GPT1 리뷰/공부/정리"
toc: true
toc_sticky: true
comments: true
permalink: /project/nlp/gpt1/
categories:
  - NLP
tags:
  - Language Modeling
  - Need to Complete

use_math: true
last_modified_at: 2020-10-16
---

이번 스터디 순서는 GPT-1이다. 원문은 [이곳](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)을 참고하자.

Keypoint:
- *generative pre-training*
- *discriminative fine-tuning*
- *task-aware input transformation*

# 1. Introduction

- 단어 단위 이상의 정보를 unlabeled text에 얻는 것은 다음과 같은 두 가지 이유 때문에 매우 어려움
    - text representation에서 transfer learning을 위해 가장 효과적인 optimization objective가 무엇인지 불분명하다.
        - ELMo같은 LM부터, NMT, discourse coherence와 같은 다양한 task를 통해 성공적인 representation을 얻어냈다.
    - 이렇게 얻은 표현에 대해 target task로 transfer learning을 진행할 때, 가장 효과적인 방법에 대한 합의(consensus)가 없다.
        - 이에는 (1) 모델의 아키텍쳐를 변화시키거나, 트랜스퍼 러닝을 할 수 있는 보조격의 목적함수를 추가하는 방법이 있다.
- GPT-1은 language understanding task를 위한 semi-supervised approach를 탐구한다. 
    - 이를 위해 unsupervised pre-training과 supervised fine-tuning을 활용
    - 본 논문은 unlabeled large corpus와 직접 annotate한 데이터 셋 (target tasks)를 활용하는 상황을 가정
    - target tasks는 unlabeled corpus와 같은 domain에 있지 않아도 됨 
- 본 작업은 two-stage training procedure를 갖는데, 
    - 먼저 unlabeled data에 대해 language modeling objective를 사용하여 initial parameter를 얻고,
    - 이후 이를 supervised objective에 transfer한다.
- GPT-1은 transformer를 사용
- 또한 transfer learning을 위해 ***traversal-style approaches***[^1]의 task-specific input adaptation을 활용
    - 이는 구조화된 text input을 하나의 contiguous sequence of tokens로 만드는 것
    - 앞서 언급했듯 fine-tune을 효과적이고, 모델에서 최소한의 변경을 가능하게 함

[^1] ***Traversal-style Approaches:*** [원문](https://arxiv.org/pdf/1509.06664.pdf)과 같이 entailment과 같은 작업에서 premise와 hypothesis를 묶어 놓는 것을 의미
{: .notice--info}

# 2. Related Work

## Semi-supervised learning for NLP

- GPT-1이 속하는 분야로, word-level/phrase-level까지 다양하게 이용되어옴
    - e.g. word2vec, GloVe, Doc2vecv, etc
- 최근들어 unlabeled corpora을 이용해서 학습하는 방법은 최근 들어 유행하기 시작함
- 그러나 이러한 접근법들은 GPT-1이 높은 수준의 의미를 포착하려는 것과는 다르게 주로 word-level에서 transfer가 일어남

## Unsupervised pre-training

- Unsupervised pre-training는 supurvised learning objective를 수정하는 대신 좋은 시작점을 찾는 것을 목표로 하는 곳에서 semi-supervised learning의 특수한 경우
- GPT-1은 natural language inference, paraphrase dectection, story completion과 같은 다양한 범위에서의 효과가 있음
- ELMo나 다른 연구들은 pre-trained LM이나 machine translation model로 부터 hidden representation을 얻고, 이를 보조적인 feature(auxiliary feature)로 사용함
    - 이는 추가적인 parameter를 학습해야 함
    - 그러나 GPT-1은 모델의 최소한의 변경만을 요구

이 부분에서는 semi-supervised learning for NLP, Unsupervised pre-training에 대한 명확한 차이를 잘 모르겠다. 스터디를 함께 진행하시는 분이 [정리한 글](https://github.com/abooundev/nlp_paper/issues/1)이 있기는 한데, 보더라도 잘 이해가 되진 않는다. 약간 NLP에서 transfer learning이 막 시도되는 시기라서 terminology에 대한 정리가 안된 것 같기도 하다.
{: .notice--warning}

## Auxiliary training objectives

- 보조적인 unsupurvised training objective를 추가하는 것은 semi-supervised learning의 일종으로 보면 된다
    - 즉, 원래 학습 목적과는 별개로 목적함수에 추가하는 것을 의미
- 실험에서도 사용하긴 했지만, 이와 별개로 unsupervised pre-training이 target task와 연관있는 언어적 요소(linguistic aspect)를 배우는 것을 확인

BERT를 아직 배우진 않았지만 BERT의 경우에는 next sentence를 예측하거나 mask 씌운 단어를 예측한다. 이 경우 이 둘의 loss를 합치게 되는데, GPT-1에서 말하는 auxilirary는 실제 목적과는 무관하지만, loss에 추가하여 이득 보는 것을 의미한다.
{: .notice--info}


# 3. Framework

- 앞서 말했듯 LM을 배우고, fine-tune하는 형태로 진행

# 3.1 Unsupervised pre-training

