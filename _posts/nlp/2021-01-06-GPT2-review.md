---
title:  "GPT2: Language Models are Unsupervised Multitask Learners review"
excerpt: "맥락과 함께 자세히 살펴보는 GPT2 논문 리뷰/설명"
toc: true
toc_sticky: true
permalink: /project/nlp/gpt2-review/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling
use_math: true
last_modified_at: 2021-01-06
---

# Intro

GPT2는 대용량 데이터 셋과 엄청나게 큰 모델을 통해 학습한 language model로, 다양한 task에서 SOTA를 달성하며 zero-shot task trasnfer의 성공적으로 수행해냈다. GPT2 논문을 읽어보며 어떻게 이러한 task를 성공적으로 수행했는지 살펴보자. 적절하게 의역했으며, 모르는 부분은 따로 빼내어 설명을 넣었다.

본 논문은 [다음](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)에서 확인할 수 있다.

# 1. Introduction

머신러닝 시스템은 많은 파라미터를 갖는 모델 (high-capacity models), 큰 데이터 셋, 그리고 지도 학습을 조합하여 큰 성공을 거뒀다. 그러나 아직은 다루기 어렵고 데이터 분포와 task specification의 약간의 변화에도 민감하다.

현재의 머신러닝은 좁은 범위에 있어서의 전문가이지 (narrow expert), 모든 것을 다루는 제너럴리스트로 보기는 어렵다.
따라서 본 논문은 다양한 task를 수행할 수 있는 좀 더 일반적인 시스템에 대해 탐구한다.
이를 통해 결국에는 수동으로 만들고 레이블링을 할 필요가 없게 될 것이다.

머신러닝의 주 접근법은 원하는 task에 대해 맞게 행동하는 학습 셋을 모으고,
이러한 행동을 머신이 따라할 수 있게 하며,
마지막으로 IID test set에서 이의 성능을 측정하는 것이다.

이러한 접근법은 narrow expert에서는 성공적이었지만, captioning model에서의 이상한 행동이라던가, 

![image](https://user-images.githubusercontent.com/47516855/103776345-a7677080-5072-11eb-980f-324723f27b11.png){: .align-center}


